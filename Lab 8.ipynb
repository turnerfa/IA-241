{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job 1\n",
    "\n",
    "Company: RAND\n",
    "Description: Policy/Defense Analyst\n",
    "\n",
    "[job website](https://www.jobs.net/jobs/rand/en-us/job/United-States/Industrial-Organizational-Psychology-Policy-Analyst/J3W5PK6RHD9SF7W42VZ/?utm_source=google_jobs_apply&utm_medium=organic&utm_campaign=google_jobs_apply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job 2\n",
    "\n",
    "Company: Chenga Corporation\n",
    "Description: Junior FBI Analyst\n",
    "\n",
    "[job website](https://www.glassdoor.com/job-listing/jr-investigative-analyst-fbi-secret-clearance-required-chenega-JV_IC1143841_KO0,54_KE55,62.htm?jl=3082345001&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlwt        \n",
    "from collections import Counter        \n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "  \n",
    "book = xlwt.Workbook() # create a new excel file\n",
    "sheet_test = book.add_sheet('word_count') # add a new sheet\n",
    "i = 0\n",
    "sheet_test.write(i,0,'word') # write the header of the first column\n",
    "sheet_test.write(i,1,'count') # write the header of the second column\n",
    "sheet_test.write(i,2,'ratio') # write the header of the third column\n",
    "    \n",
    "with open('job1.txt','r',encoding='utf-8', errors = 'ignore') as text_word: # define the location of your txt file\n",
    "     \n",
    "    # convert all the word into lower cases\n",
    "    # filter out stop words\n",
    "    word_list = [i for i in text_word.read().lower().split() if i not in stop]\n",
    "    word_total = word_list.__len__()\n",
    "     \n",
    "    count_result =  Counter(word_list)\n",
    "    for result in count_result.most_common(10):\n",
    "        i = i+1 \n",
    "        sheet_test.write(i,0,result[0])\n",
    "        sheet_test.write(i,1,result[1])\n",
    "        sheet_test.write(i,2,(result[1]/word_total))\n",
    "    \n",
    "book.save('job1.xls')# define the location of your excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlwt        \n",
    "from collections import Counter        \n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "  \n",
    "book = xlwt.Workbook() # create a new excel file\n",
    "sheet_test = book.add_sheet('word_count') # add a new sheet\n",
    "i = 0\n",
    "sheet_test.write(i,0,'word') # write the header of the first column\n",
    "sheet_test.write(i,1,'count') # write the header of the second column\n",
    "sheet_test.write(i,2,'ratio') # write the header of the third column\n",
    "    \n",
    "with open('job2.txt','r',encoding='utf-8', errors = 'ignore') as text_word: # define the location of your txt file\n",
    "     \n",
    "    # convert all the word into lower cases\n",
    "    # filter out stop words\n",
    "    word_list = [i for i in text_word.read().lower().split() if i not in stop]\n",
    "    word_total = word_list.__len__()\n",
    "     \n",
    "    count_result =  Counter(word_list)\n",
    "    for result in count_result.most_common(10):\n",
    "        i = i+1 \n",
    "        sheet_test.write(i,0,result[0])\n",
    "        sheet_test.write(i,1,result[1])\n",
    "        sheet_test.write(i,2,(result[1]/word_total))\n",
    "    \n",
    "book.save('job2.xls')# define the location of your excel file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job 1 Wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"job1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job 2 Wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"job2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Open', 'time', 'Washington,', 'opportunities', 'Location', 'Here', 'provocative', 'Note', 'analyses', 'Positions', 'internal', 'RAND', 'qualitative', 'For', 'address', 'including', 'Monica,', 'from', 'Headquartered', 'Our', 'problems', 'K-12', 'documents', 'safety,', 'Description', 'pursue', 'knowledge', 'levels', 'collegial', 'groups', 'research,', 'Citizenship.', 'Candidates', 'Join].', 'Please', 'both', 'education,', 'years', 'ability', 'Three', 'sample', 'recommendation', 'focus', 'revenues', 'make', 'visionary', 'published', 'agencies,', 'philanthropic', 'Corporation', 'non-technical', 'Special', 'world', '[Click', 'then', 'important', '50', 'position.', 'application.', 'staff.', 'task', 'higher', 'countries', 'organization', 'interest.', 'Apply\"', 'designed', 'challenges', 'SPSS,', 'Policy', 'simultaneously', 'Position', 'projects.', 'dignity', 'issues.', 'methodological', 'journals', 'instructions', 'audiences', 'Overview', 'multi-disciplinary', 'military', 'English,', 'clearly', 'SAS,', 'range', 'software', 'writing', 'skills', 'letters', 'government', 'Excellent', 'evaluation', 'approaches', 'health', \"RAND's\", 'have', 'provided', 'healthier', 'applicants', 'devise', 'develop', 'conducting', 'nonpartisan,', 'manpower,', 'groups,', 'developing', 'healthcare,', 'Some', 'approximately', 'team', 'working', 'policy', 'closely', 'modeling,', 'interactions.', 'personnel,', 'expertise', 'more', 'annual', 'teams,', 'project', 'Required', 'your', 'impact', 'Security', 'sponsored', 'projects', 'Industrial/Organizational', 'critical', 'contributions', 'interviews', 'scientific', 'thus', 'email', 'strong', 'communicate', '(including', 'variety', 'R)', 'million.', 'respects', 'effectively', 'A', 'health,', 'objective,', 'management', 'nonprofit,', 'however,', 'acute,', 'America,', 'competency', 'has', 'innovative', 'throughout', 'solutions', 'require', 'security.', 'reports', 'orally', 'schedule.', 'rely', 'applying', 'member', 'growth,', 'required.', 'data', '1,850', 'leaders', 'psychology', 'development.', 'budgets', 'MA/MS', 'prosperous.', 'complex,', 'analysts', 'literature', 'click', 'Europe,', 'across', 'Guidelines.', 'help', 'training,', 'people', 'committed', 'diverse', 'clearance.', 'emphasize', 'topics,', '\"How', 'US', 'Therefore,', 'interest', 'Title', 'Sample', 'challenges.', 'everywhere,', 'skills;', 'presentations.', 'safer', 'international', 'helping', 'offices', 'draft', 'its', 'Writing', 'management,', 'policymakers', 'also', 'integrity', 'California,', 'analyses,', '$315', 'guide', 'sustainability,', 'North', 'employment', 'core', '5', 'relevant', 'topics', 'RANDs', 'foundations.', 'We', 'Analysts', 'using', 'commitment', 'secure,', 'analysis,', 'Pittsburgh,', 'Santa', 'keeping', 'guidelines,', 'quantitative', 'Education', 'objectivity', 'peer-reviewed', 'reviews,', 'such', 'develops', 'organizations,', 'DC', 'values', 'experience', 'Psychology', 'Qualifications', 'To', 'than', 'submit', 'Multiple', 'Experience', 'ideas;', 'Australia,', 'solving', 'communities', 'technical', 'Clearance', 'preparation', 'Requirements', 'under-researched;', 'please'}\n",
      "{'actions', 'techniques,', 'sit', 'armed', 'visits', 'Process', 'Knowledge,', 'operations,', 'individuals', 'copier,', 'particles,', '(Reasonable', 'Responsibilities:', 'school', 'Date:', 'identified', 'NICS.', 'methods.', 'Prepared', 'disposition', 'enable', 'personnel', 'Hourly,', 'organization.', 'Solutions,', 'any', 'oral', 'functions.)', '401(k)', 'position:', 'Delay', 'first', 'within', 'service', '1-8-2019', 'accordance', 'validity.', 'extreme', 'functions', 'Advocate', 'must', 'stoop,', 'weather', 'paths', 'received', 'SOPs.', 'hot', 'To:', 'Time,', 'Contact', 'issue', 'dental,', 'proper', 'Check', 'reach', 'benefits', 'program', 'military,', 'those', 'Summary:', 'customer', 'year', 'Other', 'finger,', 'determination', 'career', 'SOLUTIONS,', 'Full', 'Evaluate,', 'solutions.', 'completed', 'correct/update', 'Validate', 'monitored', 'scanning', 'contact-related', 'following', 'being', 'diploma', 'hear.', 'two', 'continuous', 'be', 'responses.', 'Operating', 'successfully', 'intelligence,', 'caustic', 'Assessment', 'action', 'persons', 'hire,', 'office', 'Friday.', 'BAT.', 'physical', 'Investigative', 'demands', 'while', 'National', 'retirement', 'met', 'loud', 'Desired', 'conditions.', 'Approved', 'Evergreen', 'agency,', 'processes,', 'perform', 'training', 'satisfactorily.)', 'employee', 'responsible', 'computer,', 'long-term', 'Benefits', 'individually', 'these', 'transaction', 'abilities', 'each', 'package.', 'essential', 'job.)', 'service,', 'short', 'offers', 'detailed', 'successfully,', 'contact,', 'FLSA', 'functioning', 'temperature-controlled', 'Queue', 'law.', 'encounters', 'clean-up', 'During', 'Section', 'While', 'regularly', 'Compose', 'reviewed', 'Analyst-FBI', 'verification', 'Active', 'corresponding', 'prioritize,', 'models,', 'Minimum', 'composing', 'objectives', 'enforcement,', 'printer,', 'airborne', 'Background', 'success', 'Secret', 'crawl.', 'noise.', 'eligibility', 'through', 'System', 'duties', 'balance', 'day', 'recommended', 'control', 'processing,', 'backgrounds', 'Complete', 'assigned.', 'handle,', 'guard,', 'administrative', 'Demands:', 'hour', 'Brady', 'Work', 'existing', 'processed', 'Numbers.', 'talk', 'Clarksburg,', 'Justice', 'Reasonable', 'biometrics.', 'Bureau', 'Non-Exempt', 'Document', 'individual', 'Intermediate', 'Identifiers', '4', 'fumes', 'investigations,', 'marked', 'submitted', 'Provide', 'accuracy.', 'update', 'will', 'frequently', 'processing', 'Location:', 'high', 'Upon', 'salary', 'Procedures', 'which', 'apply', 'need', 'communication', 'Conduct', 'specific', 'accuracy', 'information', '(SOP)', 'Jr.', 'support,', 'toxic', 'Instant', 'and/or', 'independently', 'Program', 'feel', 'Services', '(SOP).', 'appeal', 'Clearance.', 'Is', 'vision', '8-10', 'Clearance:', 'our', 'follow-up', 'checks', 'kneel,', 'assistance.', 'comments.', 'Skills', 'analyze', 'accommodations', 'flexible', 'receive', 'federal', 'immediate', 'updated', 'forth', 'frequent', 'characteristics', 'communications,', 'under', 'hands', 'Individuals', 'occasionally', 'exposed', '(FBI)', 'Status:', '25', '(BAT).', 'Regular,', 'various', 'Federal', 'move', 'Reports', 'Abilities:', 'NICS', 'arms.', 'Division', 'telephonic', 'Chenega', 'Qualifications:', 'transactions', 'description', 'FFL', 'Section.', 'Job', 'Firearms', 'systems', 'cyber', 'equipment.', 'mission', 'Originating', 'justice,', 'accounts,', 'equivalent', 'per', 'Business', 'permit', 'documentation', 'Investigation', 'possess', 'environment,', 'here', 'LLC', 'timely', 'experience.', 'Company', 'workload', 'chemicals,', 'on-the-job', 'crouch,', 'adequate', 'described', 'made', 'current', 'Will', 'NICS,', 'assigned', 'vision.', 'family', 'lift', 'needed.', '(NICS)', 'written', 'able', 'shifts', 'standards', 'performing', 'telephone,', 'incoming', 'Environment:', 'exposure', 'ensure', 'deemed', 'correspondence', 'considered.', '(reviewing', 'an', 'life,', 'Assist', 'job.', '(FFL)', 'supplied', 'criminal', 'skills.', 'WV', 'normally', 'requests', 'execute', 'contact', 'provides', 'procedures,', 'needed),', 'Sections', 'Junior', 'Monday', 'TIME', 'competitive', '*This', 'Maintain', 'document', 'transaction,', 'law', 'agencies', 'electronic', 'vision,', '(To', 'Team', 'professional', 'Standard', 'agency', 'representative', 'minimum', 'Unit', 'Physical', '5-day,', 'use', 'accountable', 'method', 'independently.', 'providing', 'firearms', 'continuously,', 'Specific', 'position)', 'records', 'up', 'Superior', 'disability,', 'Title:', 'Agency', 'purge', 'prepare', 'spending', 'plan,', 'position', 'stand;', 'Information', 'Job,', 'explosives', 'safety', 'Time', 'walk;', 'not', 'Manager', 'cold', 'assisting', 'climb', 'disabilities', '(The', 'role.', '(CJIS)', 'via', 'duty', 'pounds.', 'analytic', 'held', 'Identification', 'Licenses', 'Criminal', 'medical,'}\n"
     ]
    }
   ],
   "source": [
    "with open('job1.txt','r',encoding='utf-8', errors = 'ignore') as job1:\n",
    "    with open('job2.txt','r',encoding='utf-8', errors = 'ignore') as job2:\n",
    "        job1_str =(job1.read())\n",
    "        job2_str =(job2.read())\n",
    "        \n",
    "        job1_set = set(job1_str.split())\n",
    "        job2_set = set(job2_str.split())\n",
    "        \n",
    "        print(job1_set.difference(job2_set)) \n",
    "        print(job2_set.difference(job1_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "with open('job1.txt','r',encoding='utf-8', errors = 'ignore') as job1:\n",
    "    with open('job2.txt','r',encoding='utf-8', errors = 'ignore') as job2:\n",
    "        job1_str =(job1.read())\n",
    "        job2_str =(job2.read())\n",
    "        print(fuzz.token_sort_ratio(job1_str,job2_str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
